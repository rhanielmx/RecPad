{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(dataset, trainRatio):\n",
    "    trainSize = int(len(dataset) * trainRatio)\n",
    "    trainSet = []\n",
    "    copy = list(dataset.iloc[:,:].values)\n",
    "    while len(trainSet) < trainSize:\n",
    "        index = random.randrange(len(copy))\n",
    "        trainSet.append(copy.pop(index))\n",
    "    return [np.array(trainSet), np.array(copy)]\n",
    "\n",
    "def calc_centroids(dataset, labels):\n",
    "    dataset=pd.DataFrame(dataset)\n",
    "    centroids=[]\n",
    "    for label in labels:\n",
    "        centroids.append((list((dataset[dataset['Target']==label].iloc[:,:-1]).sum()/len(dataset[dataset['Target']==label])),label))\n",
    "    return(centroids)\n",
    "\n",
    "def calc_products(dataset,amostra,labels):\n",
    "    products=[]\n",
    "    for label in labels:\n",
    "        cov_mat=dataset[dataset.Target==label].iloc[:,:-1].cov()\n",
    "        inv_cov_mat=np.linalg.inv(cov_mat)\n",
    "        centroid = calc_centroids(dataset,[label])\n",
    "        x=amostra-centroid[0][0]\n",
    "        product=np.dot(np.dot(x,inv_cov_mat),x.transpose())\n",
    "        products.append((product, centroid[0][1]))\n",
    "    return(products)        \n",
    "\n",
    "def att_class(dataset,amostra,labels):\n",
    "    dataset=pd.DataFrame(dataset)    \n",
    "    products=calc_products(dataset,amostra,labels)\n",
    "    products.sort()\n",
    "    return(products[0][1])\n",
    "\n",
    "def make_pred(dataset,amostras):\n",
    "    labels = list(dataset.iloc[:,-1].unique())\n",
    "    preds=[]\n",
    "    for amostra in amostras:\n",
    "        preds.append(att_class(dataset, amostra,labels))\n",
    "    return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_to_remove(dataset):\n",
    "    \"\"\"\n",
    "    Calcula algumas estatísticas para a remoção de outliers do dataset.\n",
    "    \n",
    "    Input\n",
    "    ----------\n",
    "    dataset: \n",
    "    Dataset a ser preparado para remoção de outliers.\n",
    "    \n",
    "    Output\n",
    "    ----------\n",
    "    cols:\n",
    "    Lista contendo os nomes das colunas dos datasets.\n",
    "  \n",
    "    means:\n",
    "    Lista contendo as médias das colunas dos datasets.\n",
    "  \n",
    "    stds:\n",
    "    Lista contendo os desios padrões das colunas dos datasets.\n",
    "  \n",
    "    \"\"\"\n",
    "    cols, means, stds = [],[],[]\n",
    "    for col in dataset.columns:\n",
    "        try:\n",
    "            cols.append(col)\n",
    "            means.append(dataset[col].mean())\n",
    "            stds.append(dataset[col].std())\n",
    "        except TypeError:\n",
    "            means.append(np.nan)\n",
    "            stds.append(np.nan)\n",
    "            print(f'Coluna {col} possui valores em formato não númerico!')\n",
    "    return(cols, means, stds)\n",
    "\n",
    "def remove_outliers(dataset):\n",
    "    \"\"\"\n",
    "    Soma todos os elementos de um vetor, exceto o que está na posição n.\n",
    "    \n",
    "    \n",
    "    Input\n",
    "    ----------\n",
    "    dataset: \n",
    "    Dataset para remoção de outliers.\n",
    "    \n",
    "    Output\n",
    "    ----------\n",
    "    dataset:\n",
    "    O dataset da entrada, agora com os outliers removidos\n",
    "    \n",
    "    \n",
    "    Obs.: São consideradas outliers da variável x, observações além do intervalo: mean(x) ± 2*std(x).\n",
    "    \"\"\"\n",
    "    cols, means, stds = prepare_to_remove(dataset)\n",
    "    k=0\n",
    "    for col in cols:\n",
    "        dataset = dataset.loc[(dataset[col] > means[k]-2*stds[k]) & (dataset[col] < means[k]+2*stds[k])]\n",
    "        k=k+1\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento dos Dados\n",
    "<br>\n",
    "\n",
    "## Remoção de Outliers\n",
    "Antes da implementação do Classificador Quadrático em si, fazemos um tratamento nos dados para remover informações melhor o desempenho do classificador.<br>\n",
    "    \n",
    "Primeiramente carregamos o dataset diretamente do github e removemos seus outliers.\n",
    "Foram considerados outliers da característica x qualquer valor além do intervalo $\\bar x \\pm 2s_x$.<br>\n",
    "\n",
    "Onde $\\bar x$ é a média amostral de x e $s_x$ é o desvio padrão amostral de x.<br>\n",
    "\n",
    "## Importância das Características\n",
    "<p>\n",
    "Após a remoção dos outliers, separamos o conjunto em conjuntos de treino e teste e utilizamos o algoritmo classificador Random Forest nesses conjuntos para determinar a importância  de cada variável.<br>\n",
    "    \n",
    "Para um melhor escolha e menor enviesamento, foi feito um loop para calcular a importância de cada característica 50 vezes, calculando-se a partir daí a importância média de cada característica.<br>\n",
    "\n",
    "Com a importância média de cada característica, foi calculada a média desses valores e foram eliminados do modelo aquelas características que apresentavam importância abaixo da importância média.<br>\n",
    "\n",
    "Esta medida foi utilizada também para diminuir o custo computacional, tornando o algoritmo mais rápido.\n",
    "</p>\n",
    "<b> Tabela com as importâncias </b>\n",
    "\n",
    "## Análise de Correlação\n",
    "Em seguida, foi feita uma análise de correlação entre as variáveis restantes do modelo, para ver quão correlacionadas elas são.\n",
    "\n",
    "<b> Imagem com o heatmap de correlação </b>\n",
    "\n",
    "Como visto na imagem $\\ref{fig:heatmap_corr}$, todas as características MA_Detection_alpha, que representam a presença de anomalias de massa em diferentes degraus de liberdade, estão extremamente correlacionadas, então a MA_Detection_alpha-0.5 é mantida por possuir a maior importância pro modelo e as outras são removidas, pois não há necessidade de 5 características fornecendo as mesmas informações. <br>\n",
    "\n",
    "Novamente, a remoção de mais características ajuda na performance do modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Average Importance(%)\n",
      "Exudates_Detection_1                     9.218834\n",
      "MA_Detection_alpha-0.5                   8.255313\n",
      "Macula_OpticDisc_Distance                7.816744\n",
      "OpticDisc_Diameter                       7.622605\n",
      "Exudates_Detection_2                     7.464093\n",
      "Exudates_Detection_3                     7.215708\n",
      "Exudates_Detection_4                     6.785061\n",
      "MA_Detection_alpha-0.6                   6.335278\n",
      "MA_Detection_alpha-0.7                   5.788920\n",
      "MA_Detection_alpha-0.9                   5.489277\n",
      "MA_Detection_alpha-1.0                   5.439081\n",
      "MA_Detection_alpha-0.8                   5.344553\n",
      "Exudates_Detection_5                     4.951660\n",
      "Exudates_Detection_7                     4.432978\n",
      "Exudates_Detection_6                     3.856670\n",
      "Exudates_Detection_8                     3.138794\n",
      "AM/FM-based classification               0.844432\n",
      "Pre-Screening                            0.000000\n",
      "Quality_Assessment                       0.000000\n"
     ]
    }
   ],
   "source": [
    "path='https://raw.githubusercontent.com/rhanielmx/RecPad/master/messidor_features.csv'\n",
    "trainRatio=0.5\n",
    "\n",
    "data=pd.read_csv(path)\n",
    "data=remove_outliers(data)\n",
    "feature_importances=[]\n",
    "cols_names=[]\n",
    "for i in range(50):\n",
    "    train_set, test_set = splitDataset(dataset=data,trainRatio=trainRatio)\n",
    "    X_train,y_train=[train_set[i][:-1] for i in range(train_set.shape[0])],[train_set[i][-1] for i in range(train_set.shape[0])]\n",
    "    X_test,y_test=[test_set[i][:-1] for i in range(test_set.shape[0])],[test_set[i][-1] for i in range(test_set.shape[0])]\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier \n",
    "    rf = RandomForestClassifier() \n",
    "    rf.fit(X_train, y_train) \n",
    "\n",
    "    feature_importances.append(rf.feature_importances_)\n",
    "\n",
    "avg_feature_importances=sum(feature_importances)/len(feature_importances)\n",
    "avg_feature_importances=pd.DataFrame(avg_feature_importances*100,columns=['Average Importance(%)'],index=list(data.columns[:-1]))\n",
    "\n",
    "print(avg_feature_importances.sort_values('Average Importance(%)',ascending=False))\n",
    "cols_to_use=[avg_feature_importances.index[i] for i in range(len(avg_feature_importances.index)) if (avg_feature_importances.iloc[i,0]>=(avg_feature_importances).mean()[0])==True]\n",
    "cols_to_use.append('Target')\n",
    "\n",
    "data=pd.read_csv(filepath_or_buffer=path,usecols=cols_to_use)\n",
    "data=remove_outliers(data)\n",
    "\n",
    "corr_matrix = data.iloc[:,:-1].corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_remove = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "[cols_to_use.remove(col) for col in to_remove];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():    \n",
    "    for _ in range(n_rounds):\n",
    "        data=pd.read_csv(filepath_or_buffer=path,usecols=cols_to_use)\n",
    "        data=remove_outliers(data)\n",
    "        \n",
    "        train_set, test_set = splitDataset(dataset=data,trainRatio=trainRatio)\n",
    "        X_test=[test_set[i][:-1] for i in range(test_set.shape[0])]\n",
    "        y_test=[test_set[i][-1] for i in range(test_set.shape[0])]\n",
    "\n",
    "        preds=make_pred(data,np.array(X_test))      \n",
    "        \n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "        sr = 100*(cm.diagonal().sum()/cm.sum())\n",
    "        confusion_matrixes.append(cm)\n",
    "        accuracys.append(sr)\n",
    "        \n",
    "        class0_success=100*cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "        class1_success=100*cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "         \n",
    "        class0_successes.append(class0_success)\n",
    "        class1_successes.append(class1_success)\n",
    "\n",
    "        if ((_+1)%(int(n_rounds/10))==0):\n",
    "            print(f'{(_+1)*(100/n_rounds):05.2f}% concluído!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.00% concluído!\n",
      "20.00% concluído!\n",
      "30.00% concluído!\n",
      "40.00% concluído!\n",
      "50.00% concluído!\n",
      "60.00% concluído!\n",
      "70.00% concluído!\n",
      "80.00% concluído!\n",
      "90.00% concluído!\n",
      "100.00% concluído!\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "accuracys = []\n",
    "confusion_matrixes = []\n",
    "class0_successes,class1_successes=[],[]\n",
    "n_rounds=100\n",
    "\n",
    "%time main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador Quadrático\n",
    "<br>\n",
    "\n",
    "Para cada classe $C_i \\in C=\\{C_1,C_2,\\dots,C_K\\}$ calculamos o vetor centróide $m_i$ como:\n",
    "$$m_i = \\dfrac{1}{N_i}\\sum_{x\\in C_i}x$$\n",
    "\n",
    "e atribuímos uma observação a classe $i$ se, $\\forall j \\neq i$,\n",
    "\n",
    "$$(x-m_i)^T C^{-1}_i(x-m_i) < (x-m_j)^T C^{-1}_j(x-m_j)$$\n",
    "\n",
    "onde $C^{-1}_i$ denota a inversa da matriz de covariância da classe i, com $i,j \\in [1,K]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>61.897917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.702553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>57.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>60.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>62.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>66.041667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracys\n",
       "count  100.000000\n",
       "mean    61.897917\n",
       "std      1.702553\n",
       "min     57.500000\n",
       "25%     60.625000\n",
       "50%     62.083333\n",
       "75%     62.916667\n",
       "max     66.041667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracys=pd.DataFrame(data={'Accuracys':accuracys})\n",
    "Accuracys.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Succces 0</th>\n",
       "      <th>Succces 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49.669482</td>\n",
       "      <td>74.189712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.433700</td>\n",
       "      <td>2.019160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>43.983402</td>\n",
       "      <td>68.951613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.100822</td>\n",
       "      <td>72.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>49.687276</td>\n",
       "      <td>74.423368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.080428</td>\n",
       "      <td>75.240204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55.555556</td>\n",
       "      <td>79.295154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Succces 0   Succces 1\n",
       "count  100.000000  100.000000\n",
       "mean    49.669482   74.189712\n",
       "std      2.433700    2.019160\n",
       "min     43.983402   68.951613\n",
       "25%     48.100822   72.852000\n",
       "50%     49.687276   74.423368\n",
       "75%     51.080428   75.240204\n",
       "max     55.555556   79.295154"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Class_Sucesses=pd.DataFrame(data={'Succces 0':class0_successes,'Succces 1':class1_successes})\n",
    "Class_Sucesses.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
